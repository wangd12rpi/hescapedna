_target_: cpgpt.model.components.model.CpGPT
d_embedding: 512
d_hidden: 512
d_dna_embedding: 1024
n_attention_heads: 16
n_layers: 32
n_mlp_blocks: 3
dropout: 0.01
architecture: transformer
activation: swiglu
positional_encoding: rotary
sample_embedding_method: cls
use_power_norm: false
fft: false
use_condition_decoder: false
condition_size: 0
use_noise_decoder: false
mlp_block_bias: false
mlp_block_norm_type: rmsnorm
mlp_block_pre_norm: false
mlp_block_post_norm: false
transformer_block_bias: false
transformer_block_norm_type: rmsnorm
transformer_block_norm_first: true
transformer_block_dropout: 0.0
