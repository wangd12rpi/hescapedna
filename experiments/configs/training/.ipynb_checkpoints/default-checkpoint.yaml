lightning:
  trainer:
    _target_: pytorch_lightning.Trainer
    _partial_: true
    max_steps: 40_000
    enable_progress_bar: true
    devices: 1  # Keep this for single GPU
    num_nodes: 1
    strategy: "auto"  # Change to 'auto' (or remove this line entirelyâ€”it defaults to single-device)
    precision: "bf16-mixed"
    log_every_n_steps: 10
    val_check_interval: 1.0
    sync_batchnorm: True  # This is fine for single GPU, but optional
    accelerator: "gpu"
    gradient_clip_val: 5
    num_sanity_val_steps: 2
    profiler: null  # Or whatever you intended; if incomplete, set to false/null to disable

train: true
test: false

# Are evaluations used at all?
evaluations:
  batch_key: id
  label_key: organ

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val_loss
    save_last: true
    mode: min
    save_top_k: 2
    dirpath: ${paths.anatomy.output}/checkpoints
    auto_insert_metric_name: True
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: val_loss
    min_delta: 1e-2
    patience: 5
    mode: min

  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: step

logger:
  csv:
    _target_: pytorch_lightning.loggers.csv_logs.CSVLogger
    save_dir: ${paths.anatomy.output}/csv_logger
    name: ""
    prefix: ""
  wandb:
    _target_: pytorch_lightning.loggers.wandb.WandbLogger
    save_dir: ${paths.anatomy.output}/wandb_logger
    log_model: False
    project: "clip_pretrain"
    name: null